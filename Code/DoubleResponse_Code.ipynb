{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# double code\n",
        "\n",
        "\n",
        "\n",
        "- v = drift rate\n",
        "- a = boundary separation\n",
        "- Ter = non-decision time\n",
        "- lamda = leakage rate\n",
        "- beta = amount of inhibition\n",
        "- al_n = number of alternative responses"
      ],
      "metadata": {
        "id": "vbYUk0PA40A-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDM\n",
        "\n",
        "**parameters we need here:**\n",
        "\n",
        "- v = drift rate\n",
        "- a = boundary separation\n",
        "- Ter = non-decision time\n"
      ],
      "metadata": {
        "id": "EyCTXm4MEZna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import all needed packages\n",
        "\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import keras\n",
        "import ipynbname\n",
        "import bayesflow as bf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "HY0b2pbiGovb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "collapsed": true,
        "outputId": "04ca7060-f17c-4f80-f7bd-ef88492f7e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bayesflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4b4d0ed0f854>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"KERAS_BACKEND\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"tensorflow\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbayesflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bayesflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defing the context\n",
        "\n",
        "# selecting possible amounts of n -> for our example we could use a fixed value\n",
        "# of n because all trials are 10000\n",
        "\n",
        "def context(n=None):\n",
        "    if n is None:\n",
        "        n = np.random.randint(2, 251)\n",
        "    return dict(n=n)"
      ],
      "metadata": {
        "id": "YTqHY05qEBVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the priors\n",
        "\n",
        "def prior(drift=None, boundary=None, starting_point=None, ndt=None):\n",
        "    if drift is None:\n",
        "        drift=np.random.dirichlet([2, 2])\n",
        "        drift=np.random.gamma(shape=5, scale=0.5) * drift\n",
        "    if boundary is None:                    #give it a feasible boundary\n",
        "        boundary=np.random.gamma(shape=3, scale=1)\n",
        "    if starting_point is None:\n",
        "        starting_point=np.random.gamma(shape=7, scale=0.1)\n",
        "    if ndt is None:\n",
        "        ndt=np.random.exponential(15)\n",
        "    else:\n",
        "        ndt=ndt.item()\n",
        "    return dict(drift=drift, boundary=boundary, starting_point=starting_point, ndt=ndt)\n"
      ],
      "metadata": {
        "id": "KsjC7AE0439w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@nb.jit(nopython=True, cache=True)\n",
        "def trial(drift, starting_point, boundary, ndt, max_t, max_drt=0.25, s=1, dt=None):\n",
        "    drift = np.asarray(drift, dtype=np.float64)  # Convert before passing to JIT\n",
        "    response = -1\n",
        "    rt = -1\n",
        "\n",
        "    if dt is None:\n",
        "        dt = max_t / 10_000.0  # Ensure float division\n",
        "\n",
        "    t = 0\n",
        "    start = float(starting_point)  # Ensure float type\n",
        "    evidence = np.random.uniform(0, start, size=len(drift))\n",
        "\n",
        "    boundary += start  # Adjust boundary based on start\n",
        "    dr = False  # Initialize double response\n",
        "\n",
        "    # Initial evidence accumulation\n",
        "    while np.all(evidence < boundary) and t < max_t:\n",
        "        for resp in range(len(drift)):  # Normal loop (prange if parallel)\n",
        "            evidence[resp] += dt * drift[resp] + np.random.normal(0, np.sqrt(s**2 * dt))\n",
        "        t += dt\n",
        "\n",
        "    rt = t + ndt\n",
        "    drt = 0\n",
        "    response_arr = np.where(evidence > boundary)[0]  # Avoid tuple issue\n",
        "\n",
        "    if response_arr.size > 0:\n",
        "        response = response_arr[0]  # Take first element\n",
        "    else:\n",
        "        response = -1  # Default\n",
        "\n",
        "    # Double response accumulation\n",
        "    while drt < max_drt and not dr:\n",
        "        for resp in range(len(drift)):\n",
        "            if response != -1 and resp != response:\n",
        "                evidence[resp] += dt * drift[resp] + np.random.normal(0, s) * np.sqrt(dt)\n",
        "                if evidence[resp] >= boundary:\n",
        "                    dr = True\n",
        "                    break\n",
        "        drt += dt  # Only increase while dr is False\n",
        "\n",
        "    return rt, response, dr, drt"
      ],
      "metadata": {
        "id": "uRxNSRpPeiIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate data for n trials\n",
        "# keep the data.shape always to max_n\n",
        "# the rest is filled with 0s\n",
        "\n",
        "\n",
        "# insert dt and drt as a liklyhood\n",
        "\n",
        "def likelihood(n, drift, boundary, starting_point, ndt, max_t=3.0, dt=0.02, max_n=300):\n",
        "    rt = np.zeros(max_n)\n",
        "    response = np.zeros(max_n)\n",
        "    observed = np.zeros(max_n)\n",
        "    dr=np.zeros(max_n)\n",
        "    drt=np.zeros(max_n)\n",
        "\n",
        "    for i in range(n):\n",
        "        rt[i], response[i], dr[i], drt[i] = trial(drift, boundary, starting_point, ndt, max_t, dt=dt)\n",
        "        observed[i]=1\n",
        "\n",
        "    return dict(rt=rt, response=response, observed=observed, dr=dr, drt=drt)"
      ],
      "metadata": {
        "id": "Jj6TunK8GHzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sufficient statistics: mean, sd, n\n",
        "def summary(rt):\n",
        "    return dict(\n",
        "        mean = np.mean(rt),\n",
        "        sd = np.std(rt)\n",
        "    )"
      ],
      "metadata": {
        "id": "cXNz3CvvcC2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creaate the simulator\n",
        "\n",
        "simulator_graph = bf.make_simulator([context, prior, likelihood, summary])\n",
        "simulator = bf.make_simulator([context, prior, likelihood])"
      ],
      "metadata": {
        "id": "z4gHqvQsGg6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = simulator_graph.sample((1000,))"
      ],
      "metadata": {
        "id": "2IiF4D7OH8uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=bf.diagnostics.pairs_samples(\n",
        "    df,\n",
        "    variable_keys=[\"drift\", \"boundary\", \"starting_point\", \"ndt\", \"mean\", \"sd\"],\n",
        "    variable_names=[r\"$\\nu_0$\", r\"$\\nu_1$\", r\"$\\alpha$\", \"starting point\",\"Ter\", \"mean RT\", \"sd RT\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "8_BrOx1kH-uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an adapter\n",
        "\n",
        "adapter = (bf.Adapter()\n",
        "    .as_set([\"rt\", \"response\", \"dr\", \"drt\", \"observed\"]) # dr -> is there double response # drt -> double rt # response -> initial true or false # observed ->\n",
        "    .constrain([\"starting_point\", \"boundary\", \"ndt\"], lower=0)\n",
        "    .standardize(include=\"drift\",    mean= 0.7, std=1.2)\n",
        "    .standardize(include=\"boundary\", mean= 0.5, std=0.7)\n",
        "    .standardize(include=\"ndt\",   mean=-2.5, std=1.3)\n",
        "    .concatenate([\"drift\", \"boundary\", \"ndt\",\"starting_point\"], into=\"inference_variables\")\n",
        "    .concatenate([\"rt\", \"response\", \"dr\", \"drt\", \"observed\"], into=\"summary_variables\")\n",
        "    .rename(\"n\", \"inference_conditions\")\n",
        ")"
      ],
      "metadata": {
        "id": "oYMZLVrzG07_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = bf.BasicWorkflow(\n",
        "    simulator = simulator,\n",
        "    adapter = adapter,\n",
        "    inference_network = bf.networks.CouplingFlow(\n",
        "        permutation=\"swap\",\n",
        "        subnet_kwargs=dict(dropout=False)\n",
        "    ),\n",
        "    summary_network=bf.networks.DeepSet(\n",
        "        base_distribution=\"normal\",\n",
        "        dropout=False\n",
        "    ),\n",
        "    inference_variables = [\"drift\", \"boundary\", \"ndt\",\"starting_point\"],\n",
        "    inference_conditions = [\"n\"],\n",
        "    summary_variables = [\"rt\", \"response\", \"dr\", \"drt\", \"observed\"]\n",
        ")"
      ],
      "metadata": {
        "id": "bba4C_vKRclk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = simulator.sample(200)\n",
        "validation_data = simulator.sample(100)"
      ],
      "metadata": {
        "id": "vD6DrD0PVuNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=workflow.fit_offline(\n",
        "    data=train_data,\n",
        "    epochs=50,\n",
        "    batch_size=250,\n",
        "    validation_data=validation_data\n",
        ")\n"
      ],
      "metadata": {
        "id": "BVnB2jgAGjHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = simulator.sample(1_000)\n",
        "plots=workflow.plot_default_diagnostics(test_data=test_data)"
      ],
      "metadata": {
        "id": "Hvn5_IxHdUO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed-Forward Inhibition"
      ],
      "metadata": {
        "id": "UhTnyu1f-9aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generation one trial\n",
        "\n",
        "# the evidence function is implemeted in this function\n",
        "\n",
        "def FFI_trial(drift, starting_point, boundary, ndt, beta, max_t, max_drt=0.25, s=1, dt=None):\n",
        "    drift = np.array(drift)\n",
        "    response = -1\n",
        "    rt = -1\n",
        "\n",
        "    if dt is None:\n",
        "        dt = max_t / 10_000\n",
        "\n",
        "    t = 0\n",
        "    evidence = np.random.uniform(0, starting_point, size=len(drift))\n",
        "\n",
        "    boundary += starting_point  # Adjust boundary based on start\n",
        "    dr = False  # Initialize double response\n",
        "\n",
        "    # Initial evidence accumulation\n",
        "    while all(evidence < boundary) and t < max_t:\n",
        "        sd = np.random.normal(0,s, size= len(drift))\n",
        "        for resp, drift_rate in enumerate(drift):\n",
        "            evidence[resp] += dt * (drift_rate - np.sum(beta * drift[~resp])) + \\\n",
        "                (sd[resp] - beta * np.sum(beta * sd[~resp])) * np.sqrt(dt)\n",
        "        t += dt\n",
        "\n",
        "    rt = t + ndt\n",
        "    drt = 0\n",
        "    response = np.argmax(evidence)\n",
        "\n",
        "    # Double response accumulation\n",
        "    while drt < max_drt:\n",
        "        sd = np.random.normal(0,s, size= len(drift))\n",
        "        for resp, drift_rate in enumerate(drift):\n",
        "            if resp != response:\n",
        "                evidence[resp] += dt * (drift_rate - np.sum(beta * drift[~resp])) + \\\n",
        "                    (sd[resp] - beta * np.sum(beta * sd[~resp])) * np.sqrt(dt)\n",
        "                if evidence[resp] >= boundary:\n",
        "                    dr = True\n",
        "                    break\n",
        "        drt += dt\n",
        "\n",
        "    return rt, response, dr, drt"
      ],
      "metadata": {
        "id": "6J23Pels-iTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lateral Inhibition"
      ],
      "metadata": {
        "id": "lBCG5T4R_Dl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generation one trial\n",
        "\n",
        "# the evidence function is implemeted in this function\n",
        "\n",
        "def LI_trial(drift, starting_point, boundary, ndt, beta, max_t, max_drt=0.25, s=1, dt=None):\n",
        "    drift = np.array(drift)\n",
        "    response = -1\n",
        "    rt = -1\n",
        "\n",
        "    if dt is None:\n",
        "        dt = max_t / 10_000\n",
        "\n",
        "    t = 0\n",
        "    evidence = np.random.uniform(0, starting_point, size=len(drift))\n",
        "\n",
        "    boundary += starting_point  # Adjust boundary based on start\n",
        "    dr = False  # Initialize double response\n",
        "\n",
        "    # Initial evidence accumulation\n",
        "    while all(evidence < boundary) and t < max_t:\n",
        "        sd = np.random.normal(0,s, size= len(drift))\n",
        "        for resp, drift_rate in enumerate(drift):\n",
        "            evidence[resp] += dt * (drift_rate - np.sum(beta * evidence[~resp])) + \\\n",
        "                sd[resp] * np.sqrt(dt)\n",
        "        t += dt\n",
        "\n",
        "    rt = t + ndt\n",
        "    drt = 0\n",
        "    response = np.argmax(evidence)\n",
        "\n",
        "    # Double response accumulation\n",
        "    while drt < max_drt:\n",
        "        sd = np.random.normal(0,s, size= len(drift))\n",
        "        for resp, drift_rate in enumerate(drift):\n",
        "            if resp != response:\n",
        "                evidence[resp] += dt * (drift_rate - np.sum(beta * evidence[~resp])) + \\\n",
        "                    sd[resp] * np.sqrt(dt)\n",
        "                if evidence[resp] >= boundary:\n",
        "                    dr = True\n",
        "                    break\n",
        "        drt += dt\n",
        "\n",
        "    return rt, response, dr, drt"
      ],
      "metadata": {
        "id": "NtLThrH2-mZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LCA\n",
        "\n",
        "- drift = nu = v = drift rate\n",
        "- boundary= alpha = a = boundary separation\n",
        "- ndt = tau = Ter = non-decision time\n",
        "- lamda = leakage rate (decay)\n",
        "- beta = amount of inhibition\n",
        "- al_n = number of alternative responses"
      ],
      "metadata": {
        "id": "pixdIZPQ_M04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import all needed packages\n",
        "\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import keras\n",
        "import ipynbname\n",
        "import bayesflow as bf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "h1b9BBx1_Q5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defing the context\n",
        "\n",
        "# selecting possible amounts of n -> for our example we could use a fixed value\n",
        "# of n because all trials are 10000\n",
        "\n",
        "def context(n=None):\n",
        "    if n is None:\n",
        "        n = np.random.randint(2, 251)\n",
        "    return dict(n=n)\n"
      ],
      "metadata": {
        "id": "Yy39Q_TF_Tq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the priors\n",
        "\n",
        "# adding lambda (leakage) and beta (inhbition)\n",
        "\n",
        "def prior(drift=None, boundary=None, starting_point=None, ndt=None, lamda = None, beta = None):\n",
        "    if drift is None:\n",
        "        drift=np.random.dirichlet([2, 2])\n",
        "        drift=np.random.gamma(shape=5, scale=0.5) * drift\n",
        "    if boundary is None:                    #give it a feasible boundary\n",
        "        boundary=np.random.gamma(shape=3, scale=1)\n",
        "    if starting_point is None:\n",
        "        starting_point=np.random.gamma(shape=7, scale=0.1)\n",
        "    if lamda is None:\n",
        "        lamda=np.random.beta(2.5, 15)\n",
        "    if beta is None:\n",
        "        beta=np.random.beta(6.5, 25)\n",
        "    if ndt is None:\n",
        "        ndt=np.random.exponential(15)\n",
        "    else:\n",
        "        ndt=ndt.item()\n",
        "    return dict(drift=drift, boundary=boundary+starting_point, lamda=lamda, beta=beta, starting_point=starting_point, ndt=ndt)\n",
        "\n"
      ],
      "metadata": {
        "id": "e-wnHX7H_TuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def LCA_trial(drift, starting_point, boundary, ndt, beta, lamda, max_t, max_drt=0.25, s=1, dt=None):\n",
        "    drift = np.array(drift)\n",
        "    response = -1\n",
        "    rt = -1\n",
        "\n",
        "    if dt is None:\n",
        "        dt = max_t / 10_000\n",
        "\n",
        "    t = 0\n",
        "    evidence = np.random.uniform(0, starting_point, size=len(drift))\n",
        "\n",
        "    boundary += starting_point  # Adjust boundary based on start\n",
        "    dr = False  # Initialize double response\n",
        "\n",
        "\n",
        "    # Initial evidence accumulation\n",
        "    while all(evidence < boundary) and t < max_t:\n",
        "        for resp, drift_rate in enumerate(drift):\n",
        "            inhibition_effect = beta * (np.sum(evidence) - evidence[resp])\n",
        "            evidence[resp] += dt * (drift_rate - lamda * evidence[resp] - inhibition_effect) + np.random.normal(0, np.sqrt(s**2 * dt))\n",
        "            if evidence[resp] < 0:\n",
        "              evidence[resp] = 0\n",
        "\n",
        "        t += dt\n",
        "\n",
        "    rt = t + ndt\n",
        "    drt = 0\n",
        "    response = np.where(evidence > boundary)[0]  # Nimm nur das erste Element des Tuples\n",
        "\n",
        "    if response.size == 1:\n",
        "        response = response.item()  # Konvertiere in eine Zahl\n",
        "    else:\n",
        "        response = -1  # Falls leer oder mehrere Werte, setze Standardwert\n",
        "\n",
        "    # Double response accumulation\n",
        "    while drt < max_drt and not dr:  # Stoppe, wenn dr True wird\n",
        "        for resp, drift_rate in enumerate(drift):\n",
        "            if response != -1 and resp != response:\n",
        "                inhibition_effect = beta * (np.sum(evidence) - evidence[resp])\n",
        "                evidence[resp] += dt * (drift_rate - lamda * evidence[resp] - inhibition_effect) + np.random.normal(0, np.sqrt(s**2 * dt))\n",
        "                if evidence[resp] < 0:\n",
        "                  evidence[resp] = 0\n",
        "                if evidence[resp] >= boundary:\n",
        "                    dr = True\n",
        "                    break\n",
        "        drt += dt  # `drt` nur erh√∂hen, solange dr False ist\n",
        "\n",
        "\n",
        "    return rt, response, dr, drt\n",
        "\n"
      ],
      "metadata": {
        "id": "nI0MKWbq_Txf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate data for n trials\n",
        "# keep the data.shape always to max_n\n",
        "# the rest is filled with 0s\n",
        "\n",
        "\n",
        "# insert dt and drt as a liklyhood\n",
        "\n",
        "def likelihood(n, drift, boundary, starting_point, ndt, beta, lamda, max_t=3.0, dt=0.02, max_n=300):\n",
        "    rt = np.zeros(max_n)\n",
        "    response = np.zeros(max_n)\n",
        "    observed = np.zeros(max_n)\n",
        "    dr=np.zeros(max_n)\n",
        "    drt=np.zeros(max_n)\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(n):\n",
        "        rt[i], response[i], dr[i], drt[i], = trial(drift, boundary, starting_point, lamda, beta, ndt, max_t, dt=dt)\n",
        "        observed[i]=1\n",
        "\n",
        "    return dict(rt=rt, response=response, observed=observed, dr=dr, drt=drt)"
      ],
      "metadata": {
        "id": "KUBju82X_Tz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sufficient statistics: mean, sd, n\n",
        "def summary(rt):\n",
        "    return dict(\n",
        "        mean = np.mean(rt),\n",
        "        sd = np.std(rt)\n",
        "    )"
      ],
      "metadata": {
        "id": "qHpN_Aoz_T2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creaate the simulator\n",
        "\n",
        "simulator_graph = bf.make_simulator([context, prior, likelihood, summary])\n",
        "simulator = bf.make_simulator([context, prior, likelihood])"
      ],
      "metadata": {
        "id": "Ex-i56aX_T4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = simulator_graph.sample(1000)"
      ],
      "metadata": {
        "id": "JffUGL50_T7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=bf.diagnostics.pairs_samples(\n",
        "    df,\n",
        "    variable_keys=[\"drift\", \"boundary\", \"starting_point\", \"ndt\", \"beta\", \"lamda\", \"mean\", \"sd\"],\n",
        "    variable_names=[r\"$\\nu_0$\", r\"$\\nu_1$\", r\"$\\alpha$\", \"starting point\",\"Ter\",\"beta\", \"lamdba\", \"mean RT\", \"sd RT\"]\n",
        ")"
      ],
      "metadata": {
        "id": "_3486Stn_T97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an adapter\n",
        "\n",
        "adapter = (bf.Adapter()\n",
        "    .as_set([\"rt\", \"response\", \"dr\", \"drt\", \"observed\"])\n",
        "    .constrain([\"starting_point\", \"boundary\", \"ndt\",\"beta\", \"lamda\"], lower=0)\n",
        "    .standardize(include=\"drift\",    mean= 0.7, std=1.2)\n",
        "    .standardize(include=\"boundary\", mean= 0.5, std=0.7)\n",
        "    .standardize(include=\"ndt\",   mean=-2.5, std=1.3)\n",
        "    .concatenate([\"drift\", \"boundary\", \"ndt\",\"starting_point\",\"beta\", \"lamda\"], into=\"inference_variables\")\n",
        "    .concatenate([\"rt\", \"response\", \"dr\", \"drt\", \"observed\"], into=\"summary_variables\")\n",
        "    .rename(\"n\", \"inference_conditions\")\n",
        ")"
      ],
      "metadata": {
        "id": "_vzolR7t_UAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = bf.BasicWorkflow(\n",
        "    simulator = simulator,\n",
        "    adapter = adapter,\n",
        "    inference_network = bf.networks.CouplingFlow(\n",
        "        permutation=\"swap\",\n",
        "        subnet_kwargs=dict(dropout=False)\n",
        "    ),\n",
        "    summary_network=bf.networks.DeepSet(\n",
        "        base_distribution=\"normal\",\n",
        "        dropout=False\n",
        "    ),\n",
        "    inference_variables = [\"drift\", \"boundary\", \"ndt\",\"starting_point\", \"beta\", \"lamda\"],\n",
        "    inference_conditions = [\"n\"],\n",
        "    summary_variables = [\"rt\", \"response\", \"dr\", \"drt\", \"observed\"]\n",
        ")"
      ],
      "metadata": {
        "id": "o05km39L_UCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = simulator.sample(5000)\n",
        "validation_data = simulator.sample(1000)"
      ],
      "metadata": {
        "id": "Ell2lsbO_p4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=workflow.fit_offline(\n",
        "    data=train_data,\n",
        "    epochs=50,\n",
        "    batch_size=250,\n",
        "    validation_data=validation_data\n",
        ")"
      ],
      "metadata": {
        "id": "htgPxYRI_qUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = simulator.sample(1_000)\n",
        "plots=workflow.plot_default_diagnostics(test_data=test_data)"
      ],
      "metadata": {
        "id": "1wb48Alh_qX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real data application\n"
      ],
      "metadata": {
        "id": "jr71Am6NGeg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_inference = pd.read_csv(\"s1.csv\")"
      ],
      "metadata": {
        "id": "5_7C60sIGdcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "#               INFERENCE ON REAL DATA               #\n",
        "######################################################\n",
        "\n",
        "data_inference = pd.read_csv(\"s1.csv\")\n",
        "\n",
        "data_inference_grouped = data_inference.groupby([\"subject\", \"condition\"])\n",
        "\n",
        "data_inference_dict = {\n",
        "    key: np.array([group[key].values.reshape(10000, 1) for _, group in data_inference_grouped])\n",
        "             for key in ['rt', 'response', 'observed']}\n",
        "\n",
        "data_inference_dict[\"n\"] = np.sum(data_inference_dict[\"observed\"], axis=1)\n",
        "\n",
        "posterior_samples = workflow.sample(conditions=data_inference_dict, num_samples=1_000)\n",
        "\n",
        "posterior = {key: value[0] for key, value in posterior_samples.items()}\n",
        "data = data_inference_grouped.get_group(('s1', 'speed'))\n",
        "\n"
      ],
      "metadata": {
        "id": "dj2CuB9JGqo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLOTS\n",
        "# paired plots\n",
        "f=bf.diagnostics.pairs_posterior(estimates=posterior) # should return paired plots\n",
        "\n",
        "\n",
        "# ecdf plot\n",
        "def ecdf(rt, response, observed, **kwargs):\n",
        "    observed_mask = (observed == 1)\n",
        "    response_0_mask = ((response == 0) & observed_mask)\n",
        "    response_1_mask = ((response == 1) & observed_mask)\n",
        "\n",
        "    response_0_prop = np.sum(response_0_mask) / np.sum(observed_mask)\n",
        "    response_1_prop = np.sum(response_1_mask) / np.sum(observed_mask)\n",
        "\n",
        "    response_0_ecdf = stats.ecdf(rt[response_0_mask]).cdf\n",
        "    response_0_ecdf = response_0_prop * response_0_ecdf.evaluate(np.linspace(0, 1, 101))\n",
        "\n",
        "    response_1_ecdf = stats.ecdf(rt[response_1_mask]).cdf\n",
        "    response_1_ecdf = response_1_prop * response_1_ecdf.evaluate(np.linspace(0, 1, 101))\n",
        "\n",
        "    return response_0_ecdf, response_1_ecdf\n",
        "\n",
        "plot_data = ecdf(**data)\n",
        "\n",
        "\n",
        "# some other plot (line plot i think)\n",
        "posterior_predictives = simulator.sample(1000, **posterior)\n",
        "\n",
        "plot_data_predictive = []\n",
        "for i in range(1000):\n",
        "    x = { key: value[i:i+1,...] for key, value in posterior_predictives.items()}\n",
        "    plot_data_predictive.append(ecdf(**x))\n",
        "plot_data_predictive = np.array(plot_data_predictive)\n",
        "\n",
        "\n",
        "plot_data_quantiles = np.quantile(\n",
        "    plot_data_predictive,\n",
        "    q = [0.25, 0.5, 0.75],\n",
        "    axis=0\n",
        ")\n",
        "plot_data_quantiles.shape\n",
        "\n",
        "t = np.linspace(0, 1, 101)\n",
        "cols = [\"red\", \"blue\"]\n",
        "for i, lab in enumerate([\"Incorrect\", \"Correct\"]):\n",
        "    plt.plot(t, plot_data[i], label=lab, color=cols[i])\n",
        "    plt.plot(t, plot_data_quantiles[1, i, :], color=cols[i], alpha=0.5, label=\"median predictive\")\n",
        "    plt.fill_between(\n",
        "        t,\n",
        "        plot_data_quantiles[0,  i,:],\n",
        "        plot_data_quantiles[-1, i,:],\n",
        "        label=\"50% predictive interval\",\n",
        "        color=cols[i],\n",
        "        alpha=0.3\n",
        "    )\n",
        "f=plt.legend()"
      ],
      "metadata": {
        "id": "CnlHc_y-I5ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_inference = pd.read_csv(\"s1.csv\")\n",
        "data_inference['condition'].unique()\n",
        "data_inference_grouped = data_inference.groupby([\"subject\", \"condition\"])\n",
        "data_inference_dict = {\n",
        "    key: np.array([group[key].values.reshape(10000, 1) for _, group in data_inference_grouped])\n",
        "             for key in ['rt', 'response','dr', 'drt', 'observed']}\n",
        "data_inference_dict[\"n\"] = np.sum(data_inference_dict[\"observed\"], axis=1)\n",
        "print({key: value.shape for key, value in data_inference_dict.items()})\n",
        "posterior_samples = workflow.sample(conditions=data_inference_dict, num_samples=1_000)\n",
        "# pick the first participant, first condition\n",
        "posterior = {key: value[0] for key, value in posterior_samples.items()}\n",
        "data = data_inference_grouped.get_group(('s1', 'speed'))\n",
        "f=bf.diagnostics.pairs_posterior(estimates=posterior)\n",
        "def ecdf(rt, response, observed, **kwargs):\n",
        "    observed_mask = (observed == 1)\n",
        "    response_0_mask = ((response == 0) & observed_mask)\n",
        "    response_1_mask = ((response == 1) & observed_mask)\n",
        "\n",
        "    response_0_prop = np.sum(response_0_mask) / np.sum(observed_mask)\n",
        "    response_1_prop = np.sum(response_1_mask) / np.sum(observed_mask)\n",
        "\n",
        "    response_0_ecdf = stats.ecdf(rt[response_0_mask]).cdf\n",
        "    response_0_ecdf = response_0_prop * response_0_ecdf.evaluate(np.linspace(0, 1, 101))\n",
        "\n",
        "    response_1_ecdf = stats.ecdf(rt[response_1_mask]).cdf\n",
        "    response_1_ecdf = response_1_prop * response_1_ecdf.evaluate(np.linspace(0, 1, 101))\n",
        "\n",
        "    return response_0_ecdf, response_1_ecdf\n",
        "\n",
        "plot_data = ecdf(**data)\n",
        "posterior_predictives = simulator.sample(1000, **posterior)\n",
        "plot_data_predictive = []\n",
        "for i in range(1000):\n",
        "    x = { key: value[i:i+1,...] for key, value in posterior_predictives.items()}\n",
        "    plot_data_predictive.append(ecdf(**x))\n",
        "plot_data_predictive = np.array(plot_data_predictive)\n",
        "plot_data_quantiles = np.quantile(\n",
        "    plot_data_predictive,\n",
        "    q = [0.25, 0.5, 0.75],\n",
        "    axis=0\n",
        ")\n",
        "plot_data_quantiles.shape\n",
        "t = np.linspace(0, 2, 101)\n",
        "cols = [\"red\", \"blue\"]\n",
        "for i, lab in enumerate([\"Incorrect\", \"Correct\"]):\n",
        "    plt.plot(t, plot_data[i], label=lab, color=cols[i])\n",
        "    plt.plot(t, plot_data_quantiles[1, i, :], color=cols[i], alpha=0.5, label=\"median predictive\")\n",
        "    plt.fill_between(\n",
        "        t,\n",
        "        plot_data_quantiles[0,  i,:],\n",
        "        plot_data_quantiles[-1, i,:],\n",
        "        label=\"50% predictive interval\",\n",
        "        color=cols[i],\n",
        "        alpha=0.3\n",
        "    )\n",
        "f=plt.legend()"
      ],
      "metadata": {
        "id": "Mev1gADW9jFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_inference = pd.read_csv(\"s1.csv\")"
      ],
      "metadata": {
        "id": "8uJjukm630p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_inference['condition'].unique()"
      ],
      "metadata": {
        "id": "c32aWR2w34t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_inference_grouped = data_inference.groupby([\"subject\", \"condition\"])"
      ],
      "metadata": {
        "id": "YKMDueg938MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_inference_dict = {\n",
        "    key: np.array([group[key].values.reshape(10000, 1) for _, group in data_inference_grouped])\n",
        "             for key in ['rt', 'response','dr', 'drt', 'observed']}\n",
        "data_inference_dict[\"n\"] = np.sum(data_inference_dict[\"observed\"], axis=1)\n",
        "print({key: value.shape for key, value in data_inference_dict.items()})"
      ],
      "metadata": {
        "id": "9zJAIDoE3_0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_samples = workflow.sample(conditions=data_inference_dict, num_samples=1000)"
      ],
      "metadata": {
        "id": "MGzINGrr4BTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick the first participant, first condition\n",
        "posterior = {key: value[0] for key, value in posterior_samples.items()}\n",
        "posterior['drift'].shape"
      ],
      "metadata": {
        "id": "9JhaDk7z4Lqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "priors = dict(drift=test_data[\"drift\"], boundary=test_data[\"boundary\"], ndt=test_data[\"ndt\"], starting_point=test_data[\"starting_point\"])\n",
        "f=bf.diagnostics.pairs_posterior(estimates=posterior)"
      ],
      "metadata": {
        "id": "hWQJrz414OXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ecdf(rt, response, observed, **kwargs):\n",
        "    observed_mask = (observed == 1)\n",
        "    response_0_mask = ((response == 0) & observed_mask)\n",
        "    response_1_mask = ((response == 1) & observed_mask)\n",
        "\n",
        "    response_0_prop = np.sum(response_0_mask) / np.sum(observed_mask)\n",
        "    response_1_prop = np.sum(response_1_mask) / np.sum(observed_mask)\n",
        "\n",
        "    response_0_ecdf = stats.ecdf(rt[response_0_mask]).cdf\n",
        "    response_0_ecdf = response_0_prop * response_0_ecdf.evaluate(np.linspace(0, 1, 101))\n",
        "\n",
        "    response_1_ecdf = stats.ecdf(rt[response_1_mask]).cdf\n",
        "    response_1_ecdf = response_1_prop * response_1_ecdf.evaluate(np.linspace(0, 1, 101))\n",
        "\n",
        "    return response_0_ecdf, response_1_ecdf"
      ],
      "metadata": {
        "id": "BgH6VuGO4P3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_inference_grouped.get_group(('s1', 'speed'))\n",
        "plot_data = ecdf(**data)"
      ],
      "metadata": {
        "id": "RJEHNALp4VuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_predictives = simulator.sample(batch_shape=1000, posterior=posterior)"
      ],
      "metadata": {
        "id": "-ElFxH8C4Yvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data_predictive = []\n",
        "for i in range(1000):\n",
        "    x = { key: value[i:i+1,...] for key, value in posterior_predictives.items()}\n",
        "    plot_data_predictive.append(ecdf(**x))\n",
        "\n",
        "plot_data_predictive = np.array(plot_data_predictive)"
      ],
      "metadata": {
        "id": "XJF7oHii4bvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute quantiles\n",
        "plot_data_quantiles = np.quantile(plot_data_predictive, q=[0.25, 0.5, 0.75], axis=0)\n",
        "plot_data_quantiles.shape"
      ],
      "metadata": {
        "id": "wZ73Viqa4di6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results\n",
        "t = np.linspace(0, 2, 101)\n",
        "cols = [\"red\", \"blue\"]\n",
        "for i, lab in enumerate([\"Incorrect\", \"Correct\"]):\n",
        "    plt.plot(t, plot_data[i], label=lab, color=cols[i])\n",
        "    plt.plot(t, plot_data_quantiles[1, i, :], color=cols[i], alpha=0.5, label=\"median predictive\")\n",
        "    plt.fill_between(t, plot_data_quantiles[0, i, :], plot_data_quantiles[-1, i, :],\n",
        "                     label=\"50% predictive interval\", color=cols[i], alpha=0.3)\n",
        "\n",
        "f = plt.legend()"
      ],
      "metadata": {
        "id": "tyDK8H1B4f1R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}